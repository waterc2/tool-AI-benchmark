import threading
import time
from concurrent.futures import ThreadPoolExecutor
from database import update_eval_scores, get_connection, get_eval_record_by_id
from llm_client import call_llm, call_all_evaluators, call_evaluator


def get_safe_result(res, key, default):
    return res.get(key, default) if isinstance(res, dict) else default


class BackgroundTaskManager:
    def __init__(self):
        self.is_running = False
        self.progress = 0.0
        self.status = "ç©ºé—²"
        self.logs = []
        self.current_case = ""
        self.total_cases = 0
        self.completed_cases = 0
        self.thread = None
        self.stop_requested = False
        self.eval_executor = ThreadPoolExecutor(max_workers=3)
        self.pending_evals = 0
        self.completed_evals = 0

    def add_log(self, msg):
        timestamp = time.strftime("%H:%M:%S")
        self.logs.append(f"[{timestamp}] {msg}")
        if len(self.logs) > 100:
            self.logs.pop(0)

    def async_evaluate_and_save(self, case, local_res, record_id):
        try:
            self.add_log(f"[å¼‚æ­¥è¯„åˆ†] å¼€å§‹è¯„åˆ†ç”¨ä¾‹: {case['title']}")
            eval_results = call_all_evaluators(case['prompt'], case['reference_answer'], local_res['content'])

            any_fail = any("è¯„å§”è°ƒç”¨åœ¨" in get_safe_result(res, 'reasoning', "") for res in eval_results.values())

            if any_fail:
                self.add_log(f"[å¼‚æ­¥è¯„åˆ†] âš ï¸ ç”¨ä¾‹ '{case['title']}' éƒ¨åˆ†è¯„åˆ†å¤±è´¥")
            else:
                scores_str = ", ".join([f"{k}: {get_safe_result(v, 'score', 0)}" for k, v in eval_results.items()])
                self.add_log(f"[å¼‚æ­¥è¯„åˆ†] ç”¨ä¾‹ '{case['title']}' è¯„åˆ†å®Œæˆ: {scores_str}")

            update_eval_scores(record_id, eval_results)
            self.add_log(f"[å¼‚æ­¥è¯„åˆ†] âœ… ç”¨ä¾‹ '{case['title']}' è¯„åˆ†å·²æ›´æ–°åˆ°æ•°æ®åº“")

        except Exception as e:
            self.add_log(f"[å¼‚æ­¥è¯„åˆ†] âŒ ç”¨ä¾‹ '{case['title']}' è¯„åˆ†å¤±è´¥: {str(e)}")
        finally:
            self.completed_evals += 1

    def async_re_evaluate(self, record_id, case_title, prompt, reference_answer, local_response, target_levels=None):
        try:
            levels_str = ", ".join(target_levels) if target_levels else "å…¨éƒ¨"
            self.add_log(f"[é‡æ–°è¯„åˆ†] å¼€å§‹è¯„åˆ†è®°å½•ID: {record_id} ({case_title}), ç›®æ ‡æ¨¡å‹: {levels_str}")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡çº§åˆ«ï¼Œåˆ™è¯„åˆ†å…¨éƒ¨
            if not target_levels:
                eval_results = call_all_evaluators(prompt, reference_answer, local_response)
            else:
                # è·å–ç°æœ‰è¯„åˆ†è®°å½•ä»¥åˆå¹¶
                existing_record = get_eval_record_by_id(record_id)
                eval_results = {}
                
                # åˆå§‹åŒ–ç°æœ‰å€¼
                for level in ["super", "high", "low"]:
                    eval_results[level] = {
                        "score": existing_record.get(f'eval_score_{level}', 0),
                        "reasoning": existing_record.get(f'eval_comment_{level}', "")
                    }
                
                # ä»…é’ˆå¯¹æŒ‡å®šçº§åˆ«å¹¶è¡Œè°ƒç”¨è¯„å§”
                from concurrent.futures import ThreadPoolExecutor as EvalExecutor
                with EvalExecutor(max_workers=3) as executor:
                    futures = {level: executor.submit(call_evaluator, prompt, reference_answer, local_response, level) 
                               for level in target_levels}
                    for level, future in futures.items():
                        try:
                            eval_results[level] = future.result()
                        except Exception as e:
                            eval_results[level] = {"score": 0, "reasoning": f"è¯„å§”è°ƒç”¨å¤±è´¥: {str(e)}"}

            any_fail = any("è¯„å§”è°ƒç”¨åœ¨" in get_safe_result(res, 'reasoning', "") for res in eval_results.values())

            if any_fail:
                self.add_log(f"[é‡æ–°è¯„åˆ†] âš ï¸ è®°å½• {record_id} éƒ¨åˆ†è¯„åˆ†å¤±è´¥")
            else:
                scores_str = ", ".join([f"{k}: {get_safe_result(v, 'score', 0)}" for k, v in eval_results.items()])
                self.add_log(f"[é‡æ–°è¯„åˆ†] è®°å½• {record_id} è¯„åˆ†å®Œæˆ: {scores_str}")

            update_eval_scores(record_id, eval_results)
            self.add_log(f"[é‡æ–°è¯„åˆ†] âœ… è®°å½• {record_id} è¯„åˆ†å·²æ›´æ–°åˆ°æ•°æ®åº“")

        except Exception as e:
            self.add_log(f"[é‡æ–°è¯„åˆ†] âŒ è®°å½• {record_id} è¯„åˆ†å¤±è´¥: {str(e)}")
        finally:
            self.completed_evals += 1

    def submit_re_evaluate(self, record_id, case_title, prompt, reference_answer, local_response, target_levels=None):
        self.pending_evals += 1
        self.eval_executor.submit(self.async_re_evaluate, record_id, case_title, prompt, reference_answer, local_response, target_levels)
        levels_str = ", ".join(target_levels) if target_levels else "å…¨éƒ¨"
        self.add_log(f"ğŸ”„ å·²æäº¤è®°å½• {record_id} ({case_title}) åˆ°å¼‚æ­¥é‡æ–°è¯„åˆ†é˜Ÿåˆ— (ç›®æ ‡: {levels_str})")

    def run_batch_test(self, selected_cases, api_base=None, api_key=None, model_id=None):
        print(f"\n[DEBUG] BackgroundTaskManager.run_batch_test started with {len(selected_cases)} cases")
        print(f"[DEBUG] Params: base={api_base}, model={model_id}")
        self.is_running = True
        self.stop_requested = False
        self.progress = 0.0
        self.completed_cases = 0
        self.total_cases = len(selected_cases)
        self.logs = []
        # ä¸é‡ç½®è¯„åˆ†è®¡æ•°å™¨ï¼Œå…è®¸ç´¯åŠ ï¼ˆæ”¯æŒå¹¶å‘çš„é‡æ–°è¯„åˆ†ä»»åŠ¡ï¼‰
        # self.pending_evals = 0
        # self.completed_evals = 0

        for idx, case in enumerate(selected_cases):
            if self.stop_requested:
                self.add_log("ğŸ›‘ ä»»åŠ¡è¢«ç”¨æˆ·åœæ­¢")
                break

            self.current_case = case['title']
            self.status = f"æ­£åœ¨å¤„ç† ({idx+1}/{self.total_cases}): {self.current_case}"
            self.add_log(f">>> å¼€å§‹æµ‹è¯•ç”¨ä¾‹: {self.current_case}")

            local_res = None
            try:
                self.add_log("æ­£åœ¨è¯·æ±‚ LLM...")
                local_res = call_llm(case['source_code'], case['prompt'], api_base, api_key, model_id)
                self.add_log(f"æœ¬åœ°æ¨¡å‹å“åº”æˆåŠŸ ({local_res['completion_tokens']} tokens)")

                record_data = {
                    "case_id": case['id'],
                    "model_name": local_res['model_name'],
                    "temperature": 0.0,
                    "local_response": local_res['content'],
                    "chain_of_thought": local_res['chain_of_thought'],
                    "prompt_tokens": local_res['prompt_tokens'],
                    "completion_tokens": local_res['completion_tokens'],
                    "total_time_ms": local_res['duration_ms'],
                    "tokens_per_second": local_res['tps'],
                    "prompt_tps": local_res.get('prompt_tps', 0),
                    "max_context": local_res.get('max_context', 0),
                    "eval_score": 0,
                    "eval_comment": "å¾…è¯„åˆ†",
                    "eval_score_super": 0,
                    "eval_comment_super": "å¾…è¯„åˆ†",
                    "eval_score_high": 0,
                    "eval_comment_high": "å¾…è¯„åˆ†",
                    "eval_score_low": 0,
                    "eval_comment_low": "å¾…è¯„åˆ†"
                }

                from database import save_eval_record
                record_id = save_eval_record(record_data)

                self.add_log(f"âœ… ç”¨ä¾‹ '{self.current_case}' æœ¬åœ°æµ‹è¯•å®Œæˆï¼Œå·²ä¿å­˜ (è®°å½•ID: {record_id})")

                self.pending_evals += 1
                self.eval_executor.submit(self.async_evaluate_and_save, case, local_res, record_id)
                self.add_log(f"ğŸš€ å·²æäº¤ç”¨ä¾‹ '{self.current_case}' åˆ°å¼‚æ­¥è¯„åˆ†é˜Ÿåˆ—")

            except Exception as e:
                self.add_log(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")

            self.completed_cases += 1
            self.progress = self.completed_cases / self.total_cases

        self.is_running = False
        self.status = f"æµ‹è¯•å®Œæˆï¼Œç­‰å¾…è¯„åˆ† ({self.completed_evals}/{self.pending_evals})"
        self.progress = 1.0

        self.status = "å…¨éƒ¨å®Œæˆ"
        self.add_log(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å…±æµ‹è¯• {self.total_cases} ä¸ªç”¨ä¾‹ï¼Œè¯„åˆ† {self.completed_evals} ä¸ª")

    def start_task(self, selected_cases, api_base=None, api_key=None, model_id=None):
        if not self.is_running:
            self.thread = threading.Thread(target=self.run_batch_test, args=(selected_cases, api_base, api_key, model_id,))
            self.thread.daemon = True
            self.thread.start()

    def stop_task(self):
        self.stop_requested = True
