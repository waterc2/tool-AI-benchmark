"""Evaluation history page for the LLM Benchmarker app."""

import time
import streamlit as st
from database import get_all_test_cases, get_all_models, get_eval_history, delete_eval_record

# æ¯é¡µæ˜¾ç¤ºçš„è®°å½•æ•°
RECORDS_PER_PAGE = 20


def render_history():
    """Render the evaluation history page."""
    st.header("ğŸ“œ è¯„æµ‹å†å²è®°å½•")

    df_cases = get_all_test_cases()
    col_f1, col_f2 = st.columns(2)

    case_options = ["å…¨éƒ¨"] + df_cases['title'].tolist()
    selected_case_title = col_f1.selectbox("æŒ‰ç”¨ä¾‹ç­›é€‰", case_options)

    model_options = ["å…¨éƒ¨"] + get_all_models()
    selected_model = col_f2.selectbox("æŒ‰æ¨¡å‹ç­›é€‰", model_options)

    case_id = None
    if selected_case_title != "å…¨éƒ¨":
        case_id = df_cases[df_cases['title'] == selected_case_title]['id'].iloc[0]

    df_history = get_eval_history(case_id, selected_model)

    if df_history.empty:
        st.info("æš‚æ— è¯„æµ‹è®°å½•ã€‚")
        return

    total_records = len(df_history)
    total_pages = (total_records + RECORDS_PER_PAGE - 1) // RECORDS_PER_PAGE

    # --- è‡ªåŠ¨é‡æ–°è¯„åˆ†é€»è¾‘ ---
    col_btn, col_page, _ = st.columns([1.5, 2, 2])
    
    # é¢„è®¡ç®—å¤±è´¥è®°å½•ï¼ˆä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼‰
    failed_mask = (
        (df_history.get('eval_score_super', 0) == 0) |
        (df_history.get('eval_score_high', 0) == 0) |
        (df_history.get('eval_score_low', 0) == 0)
    )
    failed_count = failed_mask.sum() if hasattr(failed_mask, 'sum') else len([x for x in failed_mask if x])
    
    if col_btn.button(f"ğŸ”„ è‡ªåŠ¨é‡æ–°è¯„åˆ† ({failed_count}æ¡)", 
                     help="å°†æ‰€æœ‰ super/high/low ä¸­åˆ†æ•°ä¸º 0 çš„æ¨¡å—é‡æ–°è¯„åˆ†"):
        if failed_count > 0:
            task_mgr = st.session_state.task_manager
            count = 0
            for row in df_history.itertuples():
                target_levels = []
                if getattr(row, 'eval_score_super', 0) == 0: target_levels.append('super')
                if getattr(row, 'eval_score_high', 0) == 0: target_levels.append('high')
                if getattr(row, 'eval_score_low', 0) == 0: target_levels.append('low')
                
                if target_levels:
                    task_mgr.submit_re_evaluate(
                        row.id,
                        row.case_title,
                        row.prompt,
                        row.reference_answer,
                        row.local_response,
                        target_levels=target_levels
                    )
                    count += 1
            st.success(f"å·²æ‰¹é‡æäº¤ {count} æ¡è®°å½•åˆ°è¯„åˆ†é˜Ÿåˆ—ï¼")
            time.sleep(1)
            st.rerun()
        else:
            st.info("æ²¡æœ‰éœ€è¦é‡æ–°è¯„åˆ†çš„è®°å½• (è¯„åˆ†é¡¹å‡é0)ã€‚")

    # åˆ†é¡µæ§åˆ¶
    current_page = col_page.number_input(
        f"é¡µç  (å…± {total_pages} é¡µ, {total_records} æ¡è®°å½•)",
        min_value=1, max_value=max(1, total_pages), value=1, step=1
    )
    
    # è®¡ç®—å½“å‰é¡µçš„æ•°æ®èŒƒå›´
    start_idx = (current_page - 1) * RECORDS_PER_PAGE
    end_idx = min(start_idx + RECORDS_PER_PAGE, total_records)
    df_page = df_history.iloc[start_idx:end_idx]

    st.write("---")
    h_col1, h_col2, h_col3, h_col4, h_col5 = st.columns([1, 2, 2, 2, 1.5])
    h_col1.write("**ID**")
    h_col2.write("**æµ‹è¯•ç”¨ä¾‹**")
    h_col3.write("**æ¨¡å‹**")
    h_col4.write("**å¾—åˆ†**")
    h_col5.write("**æ“ä½œ**")

    # ä½¿ç”¨ itertuples() æ›¿ä»£ iterrows() æå‡æ€§èƒ½
    for row in df_page.itertuples():
        record_id = row.id
        view_key = f"view_eval_{record_id}"
        is_expanded = st.session_state.get(view_key, False)
        
        with st.container(border=True):
            r_col1, r_col2, r_col3, r_col4, r_col5 = st.columns([1, 2, 2, 2, 1.5])
            r_col1.write(f"{record_id}")
            r_col2.write(f"{row.case_title}")
            r_col3.write(f"{row.model_name}")
            
            total_score = getattr(row, 'eval_score', 0) or 0
            s_score = int(getattr(row, 'eval_score_super', 0) or 0)
            h_score = int(getattr(row, 'eval_score_high', 0) or 0)
            l_score = int(getattr(row, 'eval_score_low', 0) or 0)
            r_col4.write(f"{total_score:.1f} ({s_score},{h_score},{l_score})")

            btn_label = "æ”¶èµ·" if is_expanded else "æŸ¥çœ‹è¯¦æƒ…"
            btn_col1, btn_col2 = r_col5.columns([1, 1])
            if btn_col1.button(btn_label, key=f"btn_eval_{record_id}"):
                st.session_state[view_key] = not is_expanded
                st.rerun()
            
            if btn_col2.button("ğŸ”„", key=f"re_eval_top_{record_id}", help="é‡æ–°è¯„åˆ†"):
                target_levels = []
                if s_score == 0: target_levels.append('super')
                if h_score == 0: target_levels.append('high')
                if l_score == 0: target_levels.append('low')
                
                if not target_levels:
                    target_levels = None
                    
                task_mgr = st.session_state.task_manager
                task_mgr.submit_re_evaluate(
                    record_id,
                    row.case_title,
                    row.prompt,
                    row.reference_answer,
                    row.local_response,
                    target_levels=target_levels
                )
                msg = f"å·²æäº¤è®°å½• {record_id} åˆ°å¼‚æ­¥è¯„åˆ†é˜Ÿåˆ—"
                if target_levels:
                    msg += f" (ç›®æ ‡: {', '.join(target_levels)})"
                st.success(f"{msg}ï¼")
                time.sleep(1)
                st.rerun()

            if is_expanded:
                st.divider()
                col_a, col_b = st.columns(2)
                with col_a:
                    st.write("**æœ¬åœ°æ¨¡å‹å›ç­”**")
                    st.code(row.local_response)
                    if row.chain_of_thought:
                        with st.expander("ğŸ’­ æŸ¥çœ‹æ€ç»´é“¾ (CoT)", expanded=True):
                            st.write(row.chain_of_thought)

                with col_b:
                    st.write("**è¯„å§”è¯„åˆ†ä¸ç†ç”±**")

                    c1, c2, c3 = st.columns(3)
                    c1.metric("Super", f"{s_score}/100")
                    c2.metric("High", f"{h_score}/100")
                    c3.metric("Low", f"{l_score}/100")

                    with st.expander("æŸ¥çœ‹è¯¦ç»†è¯„è¯­", expanded=True):
                        st.markdown(f"**Super:** {getattr(row, 'eval_comment_super', 'æ— ') or 'æ— '}")
                        st.markdown(f"**High:** {getattr(row, 'eval_comment_high', 'æ— ') or 'æ— '}")
                        st.markdown(f"**Low:** {getattr(row, 'eval_comment_low', 'æ— ') or 'æ— '}")

                    st.write("**æ€§èƒ½æŒ‡æ ‡**")
                    st.write(f"- è€—æ—¶: {row.total_time_ms:.2f} ms")
                    st.write(f"- ç”Ÿæˆé€Ÿåº¦: {row.tokens_per_second:.2f} tps")
                    prompt_tps = getattr(row, 'prompt_tps', 0) or 0
                    if prompt_tps > 0:
                        st.write(f"- é¢„è¯»é€Ÿåº¦: {prompt_tps:.2f} tps")
                    max_context = getattr(row, 'max_context', 0) or 0
                    if max_context > 0:
                        st.write(f"- æ¨¡å‹ä¸Šä¸‹æ–‡: {max_context} tokens")
                    st.write(f"- Tokens: {row.prompt_tokens} (in) / {row.completion_tokens} (out)")

                    st.divider()
                    if st.button("ğŸ—‘ï¸ åˆ é™¤æ­¤æ¡è®°å½•", key=f"del_eval_{record_id}"):
                        delete_eval_record(record_id)
                        st.success(f"è®°å½• {record_id} å·²åˆ é™¤")
                        st.rerun()

