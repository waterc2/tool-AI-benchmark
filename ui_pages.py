import json
import time
import pandas as pd
import streamlit as st
from streamlit_autorefresh import st_autorefresh
from database import (
    get_stats,
    get_all_test_cases,
    delete_test_case,
    save_test_case
)


def render_sidebar(task_mgr):
    with st.sidebar:
        st.title("ğŸš€ LLM Benchmarker")
        menu = st.radio("èœå•", ["ç”¨ä¾‹ç®¡ç†", "æ‰§è¡Œæµ‹è¯•", "å†å²è®°å½•", "ç»Ÿè®¡åˆ†æ"])

        if task_mgr.is_running or task_mgr.pending_evals > task_mgr.completed_evals:
            st.divider()
            st.subheader("â³ æ­£åœ¨æ‰§è¡Œæµ‹è¯•")
            st.info(task_mgr.status)
            st.progress(task_mgr.progress)

            if task_mgr.pending_evals > 0:
                eval_progress = min(task_mgr.completed_evals / task_mgr.pending_evals, 1.0)
                st.write(f"å¼‚æ­¥è¯„åˆ†è¿›åº¦: {task_mgr.completed_evals}/{task_mgr.pending_evals}")
                st.progress(eval_progress)

            if task_mgr.is_running and st.button("ğŸ›‘ åœæ­¢ä»»åŠ¡"):
                task_mgr.stop_task()

            st_autorefresh(interval=2000, key="progress_refresh")
        elif task_mgr.status == "å…¨éƒ¨å®Œæˆ":
            st.divider()
            st.success("âœ… æµ‹è¯•ä»»åŠ¡å·²å®Œæˆ")
            if st.button("æ¸…é™¤çŠ¶æ€"):
                task_mgr.status = "ç©ºé—²"
                task_mgr.pending_evals = 0
                task_mgr.completed_evals = 0
                st.rerun()

        st.divider()
        st.header("ğŸ“Š å…¨å±€ç»Ÿè®¡")
        stats = get_stats()
        st.metric("æµ‹è¯•ç”¨ä¾‹æ•°", stats['total_cases'])
        st.metric("æ€»è¯„æµ‹æ¬¡æ•°", stats['total_evals'])
        st.metric("å¹³å‡å¾—åˆ†", f"{stats['avg_score']:.2f}/100")
        st.metric("å¹³å‡é€Ÿåº¦", f"{stats['avg_tps']:.2f} tps")

    return menu


def render_case_manager():
    st.header("ğŸ“ æµ‹è¯•ç”¨ä¾‹ç®¡ç†")

    editing_case_id = st.session_state.get("editing_case_id", None)
    edit_data = None
    if editing_case_id:
        df_cases = get_all_test_cases()
        edit_data = df_cases[df_cases['id'] == editing_case_id].iloc[0]

    form_title = "ğŸ“ ç¼–è¾‘æµ‹è¯•ç”¨ä¾‹" if editing_case_id else "â• æ–°å»ºæµ‹è¯•ç”¨ä¾‹"
    with st.expander(form_title, expanded=(editing_case_id is not None)):
        with st.form("case_form", clear_on_submit=not editing_case_id):
            title = st.text_input("ç”¨ä¾‹æ ‡é¢˜*", value=edit_data['title'] if edit_data is not None else "", placeholder="ä¾‹å¦‚ï¼šå®ç° LRU ç¼“å­˜")
            category = st.text_input("åˆ†ç±»", value=edit_data['category'] if edit_data is not None else "", placeholder="ç®—æ³• / Web / ä¿®å¤")

            st.write("ğŸ“‚ æºä»£ç  (æ”¯æŒå¤šæ–‡ä»¶)")
            st.info("è¯·ä»¥ JSON æ ¼å¼è¾“å…¥ï¼Œä¾‹å¦‚ï¼š`{\"main.py\": \"...\"}`ã€‚å¦‚æœæ˜¯å•æ–‡ä»¶ï¼Œå¯ç›´æ¥è¾“å…¥ä»£ç ã€‚**ç•™ç©ºåˆ™è¡¨ç¤ºä»é›¶å¼€å§‹å†™æ–°åŠŸèƒ½ã€‚**")

            default_source = ""
            if edit_data is not None:
                try:
                    src_obj = json.loads(edit_data['source_code'])
                    default_source = json.dumps(src_obj, indent=2, ensure_ascii=False) if src_obj else ""
                except Exception:
                    default_source = edit_data['source_code']

            source_code_input = st.text_area("æºä»£ç å†…å®¹", value=default_source, height=200, placeholder="ç•™ç©ºè¡¨ç¤ºä»é›¶å¼€å§‹...")

            prompt = st.text_area("ä¿®æ”¹è¦æ±‚ (Prompt)*", value=edit_data['prompt'] if edit_data is not None else "", height=100)
            reference_answer = st.text_area("å‚è€ƒç­”æ¡ˆ", value=edit_data['reference_answer'] if edit_data is not None else "", height=150)

            col_btn1, col_btn2 = st.columns([1, 5])
            submit = col_btn1.form_submit_button("ä¿å­˜")
            cancel = col_btn2.form_submit_button("å–æ¶ˆç¼–è¾‘") if editing_case_id else False

            if cancel:
                st.session_state.editing_case_id = None
                st.rerun()

            if submit:
                if not title or not prompt:
                    st.error("æ ‡é¢˜å’Œè¦æ±‚æ˜¯å¿…å¡«é¡¹ï¼")
                else:
                    if not source_code_input.strip():
                        source_dict = {}
                    else:
                        try:
                            source_dict = json.loads(source_code_input)
                        except Exception:
                            source_dict = {"source": source_code_input}

                    save_test_case(title, category, source_dict, prompt, reference_answer, case_id=editing_case_id)
                    st.success(f"ç”¨ä¾‹ '{title}' å·²ä¿å­˜ï¼")
                    st.session_state.editing_case_id = None
                    st.rerun()

    st.subheader("ç°æœ‰ç”¨ä¾‹åˆ—è¡¨")
    df_cases = get_all_test_cases()
    if not df_cases.empty:
        for _, row in df_cases.iterrows():
            with st.container(border=True):
                col1, col2, col3, col4 = st.columns([4, 1, 1, 1])
                col1.write(f"**{row['title']}** ({row['category']})")
                if col2.button("æŸ¥çœ‹", key=f"view_{row['id']}"):
                    st.session_state[f"view_case_{row['id']}"] = not st.session_state.get(f"view_case_{row['id']}", False)

                if col3.button("âœï¸", key=f"edit_{row['id']}"):
                    st.session_state.editing_case_id = row['id']
                    st.rerun()

                if col4.button("ğŸ—‘ï¸", key=f"del_{row['id']}"):
                    delete_test_case(row['id'])
                    if st.session_state.get("editing_case_id") == row['id']:
                        st.session_state.editing_case_id = None
                    st.rerun()

                if st.session_state.get(f"view_case_{row['id']}", False):
                    try:
                        src_data = json.loads(row['source_code'])
                        if not src_data:
                            st.info("ğŸ’¡ æ­¤ç”¨ä¾‹ä¸ºâ€œä»é›¶å¼€å§‹â€å¼€å‘ï¼Œæ— åˆå§‹æºä»£ç ã€‚")
                        else:
                            st.json(src_data)
                    except Exception:
                        st.code(row['source_code'])

                    st.text_area("Prompt", row['prompt'], disabled=True)
                    st.text_area("å‚è€ƒç­”æ¡ˆ", row['reference_answer'], disabled=True)
    else:
        st.info("æš‚æ— ç”¨ä¾‹ï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ªã€‚")


def render_test_runner(task_mgr):
    st.header("ğŸ§ª æ‰§è¡Œè¯„æµ‹")

    if task_mgr.is_running:
        st.warning("ğŸš€ æµ‹è¯•ä»»åŠ¡æ­£åœ¨åå°è¿è¡Œä¸­...")
        st.subheader(task_mgr.status)
        st.progress(task_mgr.progress)

        if task_mgr.pending_evals > 0:
            eval_progress = min(task_mgr.completed_evals / task_mgr.pending_evals, 1.0)
            st.write(f"**å¼‚æ­¥è¯„åˆ†è¿›åº¦**: {task_mgr.completed_evals}/{task_mgr.pending_evals}")
            st.progress(eval_progress)

        with st.expander("ğŸ” æŸ¥çœ‹å®æ—¶æ‰§è¡Œæ—¥å¿—", expanded=True):
            st.code("\n".join(task_mgr.logs))

        if st.button("ğŸ›‘ åœæ­¢å½“å‰ä»»åŠ¡"):
            task_mgr.stop_task()
            st.rerun()
    else:
        if task_mgr.pending_evals > task_mgr.completed_evals:
            st.info(f"â„¹ï¸ åå°è¯„åˆ†ä»»åŠ¡è¿›è¡Œä¸­: {task_mgr.completed_evals}/{task_mgr.pending_evals}ã€‚æ‚¨å¯ä»¥ç»§ç»­å¯åŠ¨æ–°çš„æµ‹è¯•ã€‚")
            eval_progress = min(task_mgr.completed_evals / task_mgr.pending_evals, 1.0)
            st.progress(eval_progress)
            st.divider()
        
        df_cases = get_all_test_cases()
        if df_cases.empty:
            st.warning("è¯·å…ˆåœ¨'ç”¨ä¾‹ç®¡ç†'ä¸­åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ã€‚")
        else:
            st.write("é€‰æ‹©è¦æµ‹è¯•çš„ç”¨ä¾‹ï¼š")
            selected_indices = []
            for i, row in df_cases.iterrows():
                if st.checkbox(f"{row['title']} ({row['category']})", key=f"check_{row['id']}"):
                    selected_indices.append(i)

            st.divider()

            col_btn1, col_btn2 = st.columns([1, 4])
            start_batch = col_btn1.button("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•", type="primary", disabled=len(selected_indices) == 0)
            start_all = col_btn2.button("ğŸ”¥ æ‰§è¡Œå…¨éƒ¨ç”¨ä¾‹")

            if start_all:
                selected_indices = list(range(len(df_cases)))

            if start_batch or start_all:
                selected_cases = [df_cases.iloc[i].to_dict() for i in selected_indices]
                task_mgr.start_task(selected_cases)
                st.success("ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨ï¼æ‚¨å¯ä»¥åˆ‡æ¢åˆ°å…¶ä»–é¡µé¢æŸ¥çœ‹ã€‚")
                st.rerun()


def render_history():
    st.header("ğŸ“œ è¯„æµ‹å†å²è®°å½•")

    from database import get_all_models, get_eval_history, delete_eval_record

    df_cases = get_all_test_cases()
    col_f1, col_f2 = st.columns(2)

    case_options = ["å…¨éƒ¨"] + df_cases['title'].tolist()
    selected_case_title = col_f1.selectbox("æŒ‰ç”¨ä¾‹ç­›é€‰", case_options)

    model_options = ["å…¨éƒ¨"] + get_all_models()
    selected_model = col_f2.selectbox("æŒ‰æ¨¡å‹ç­›é€‰", model_options)

    case_id = None
    if selected_case_title != "å…¨éƒ¨":
        case_id = df_cases[df_cases['title'] == selected_case_title]['id'].iloc[0]

    df_history = get_eval_history(case_id, selected_model)

    if not df_history.empty:
        st.write("---")
        h_col1, h_col2, h_col3, h_col4, h_col5 = st.columns([1, 3, 2, 1, 2])
        h_col1.write("**ID**")
        h_col2.write("**æµ‹è¯•ç”¨ä¾‹**")
        h_col3.write("**æ¨¡å‹**")
        h_col4.write("**å¾—åˆ†**")
        h_col5.write("**æ“ä½œ**")

        for _, row in df_history.iterrows():
            with st.container(border=True):
                r_col1, r_col2, r_col3, r_col4, r_col5 = st.columns([1, 3, 2, 1, 2])
                r_col1.write(f"{row['id']}")
                r_col2.write(f"{row['case_title']}")
                r_col3.write(f"{row['model_name']}")
                r_col4.write(f"{row['eval_score']}")

                btn_label = "æ”¶èµ·" if st.session_state.get(f"view_eval_{row['id']}", False) else "æŸ¥çœ‹è¯¦æƒ…"
                btn_col1, btn_col2 = r_col5.columns([1, 1])
                if btn_col1.button(btn_label, key=f"btn_eval_{row['id']}"):
                    st.session_state[f"view_eval_{row['id']}"] = not st.session_state.get(f"view_eval_{row['id']}", False)
                    st.rerun()
                
                if btn_col2.button("ğŸ”„", key=f"re_eval_top_{row['id']}", help="é‡æ–°è¯„åˆ†"):
                    task_mgr = st.session_state.task_manager
                    task_mgr.submit_re_evaluate(
                        row['id'],
                        row['case_title'],
                        row['prompt'],
                        row['reference_answer'],
                        row['local_response']
                    )
                    st.success(f"å·²æäº¤è®°å½• {row['id']} åˆ°å¼‚æ­¥è¯„åˆ†é˜Ÿåˆ—ï¼")
                    time.sleep(1)
                    st.rerun()

                if st.session_state.get(f"view_eval_{row['id']}", False):
                    st.divider()
                    col_a, col_b = st.columns(2)
                    with col_a:
                        st.write("**æœ¬åœ°æ¨¡å‹å›ç­”**")
                        st.code(row['local_response'])
                        if row['chain_of_thought']:
                            with st.expander("ğŸ’­ æŸ¥çœ‹æ€ç»´é“¾ (CoT)", expanded=True):
                                st.write(row['chain_of_thought'])

                    with col_b:
                        st.write("**è¯„å§”è¯„åˆ†ä¸ç†ç”±**")

                        c1, c2, c3 = st.columns(3)
                        c1.metric("Super", f"{row.get('eval_score_super', 0)}/100")
                        c2.metric("High", f"{row.get('eval_score_high', 0)}/100")
                        c3.metric("Low", f"{row.get('eval_score_low', 0)}/100")

                        with st.expander("æŸ¥çœ‹è¯¦ç»†è¯„è¯­", expanded=True):
                            st.markdown(f"**Super:** {row.get('eval_comment_super', 'æ— ')}")
                            st.markdown(f"**High:** {row.get('eval_comment_high', 'æ— ')}")
                            st.markdown(f"**Low:** {row.get('eval_comment_low', 'æ— ')}")

                        st.write("**æ€§èƒ½æŒ‡æ ‡**")
                        st.write(f"- è€—æ—¶: {row['total_time_ms']:.2f} ms")
                        st.write(f"- ç”Ÿæˆé€Ÿåº¦: {row['tokens_per_second']:.2f} tps")
                        if 'prompt_tps' in row and row['prompt_tps'] > 0:
                            st.write(f"- é¢„è¯»é€Ÿåº¦: {row['prompt_tps']:.2f} tps")
                        if 'max_context' in row and row['max_context'] > 0:
                            st.write(f"- æ¨¡å‹ä¸Šä¸‹æ–‡: {row['max_context']} tokens")
                        st.write(f"- Tokens: {row['prompt_tokens']} (in) / {row['completion_tokens']} (out)")

                        st.divider()
                        if st.button("ğŸ—‘ï¸ åˆ é™¤æ­¤æ¡è®°å½•", key=f"del_eval_{row['id']}"):
                            delete_eval_record(row['id'])
                            st.success(f"è®°å½• {row['id']} å·²åˆ é™¤")
                            st.rerun()
    else:
        st.info("æš‚æ— è¯„æµ‹è®°å½•ã€‚")


def render_stats():
    st.header("ğŸ“Š ç»Ÿè®¡åˆ†ææŠ¥å‘Š")

    from database import (
        get_model_summary_stats,
        get_model_detail_stats,
        get_case_summary_stats,
        get_case_model_ranking,
        get_model_speed_ranking
    )

    tab1, tab2, tab3 = st.tabs(["ä»¥æ¨¡å‹ä¸ºå•ä½", "ä»¥æµ‹è¯•é¢˜ä¸ºå•ä½", "é€Ÿåº¦æ’è¡Œ"])

    with tab1:
        st.subheader("æ¨¡å‹æ€§èƒ½æ±‡æ€»")
        df_model_summary = get_model_summary_stats()

        if not df_model_summary.empty:
            df_speed = get_model_speed_ranking()

            df_model_summary = pd.merge(
                df_model_summary,
                df_speed[['model_name', 'avg_total_time_ms']],
                on='model_name',
                how='left'
            )

            for _, row in df_model_summary.iterrows():
                with st.container(border=True):
                    col1, col2, col3, col4 = st.columns([3, 2, 1, 2])
                    col1.write(f"**æ¨¡å‹: {row['model_name']}**")
                    col2.write(f"å¹³å‡åˆ†: **{row['avg_score']:.2f}** / 100")
                    col3.write(f"æµ‹è¯•æ¬¡æ•°: {row['test_count']}")

                    avg_time_ms = row['avg_total_time_ms']
                    if pd.notna(avg_time_ms):
                        avg_time_s = avg_time_ms / 1000.0
                        time_str = f"{avg_time_s:.2f} s"
                    else:
                        time_str = "N/A"
                    col4.write(f"**å¹³å‡æ€»è€—æ—¶**: {time_str}")

                    if st.button("æŸ¥çœ‹æ¯é¢˜å¹³å‡åˆ†", key=f"model_detail_{row['model_name']}"):
                        st.session_state[f"show_detail_{row['model_name']}"] = not st.session_state.get(f"show_detail_{row['model_name']}", False)
                        st.rerun()

                    if st.session_state.get(f"show_detail_{row['model_name']}", False):
                        st.write("---")
                        df_details = get_model_detail_stats(row['model_name'])
                        
                        display_df = df_details.copy()
                        display_df['avg_total_time_s'] = display_df['avg_total_time_ms'].apply(
                            lambda x: f"{x/1000:.2f}" if pd.notna(x) else "N/A"
                        )
                        display_df['avg_completion_tokens'] = display_df['avg_completion_tokens'].apply(
                            lambda x: f"{x:.1f}" if pd.notna(x) else "N/A"
                        )
                        display_df['avg_tps'] = display_df['avg_tps'].apply(
                            lambda x: f"{x:.2f}" if pd.notna(x) else "N/A"
                        )
                        display_df['avg_prompt_tps'] = display_df['avg_prompt_tps'].apply(
                            lambda x: f"{x:.2f}" if pd.notna(x) and x > 0 else "N/A"
                        )
                        
                        st.dataframe(
                            display_df[[
                                'case_title', 'avg_score', 'run_count',
                                'avg_score_super', 'avg_score_high', 'avg_score_low',
                                'avg_completion_tokens',
                                'avg_total_time_s', 'avg_tps', 'avg_prompt_tps'
                            ]].rename(columns={
                                'case_title': 'æµ‹è¯•é¢˜',
                                'avg_score': 'ç»¼åˆå¹³å‡åˆ†',
                                'run_count': 'è¿è¡Œæ¬¡æ•°',
                                'avg_score_super': 'Superè¯„åˆ†',
                                'avg_score_high': 'Highè¯„åˆ†',
                                'avg_score_low': 'Lowè¯„åˆ†',
                                'avg_completion_tokens': 'è¾“å‡ºTokens',
                                'avg_total_time_s': 'å¹³å‡è€—æ—¶(s)',
                                'avg_tps': 'ç”Ÿæˆé€Ÿåº¦(tps)',
                                'avg_prompt_tps': 'é¢„è¯»é€Ÿåº¦(tps)'
                            }),
                            hide_index=True,
                            width='stretch'
                        )
        else:
            st.info("æš‚æ— æ¨¡å‹ç»Ÿè®¡æ•°æ®ã€‚")

    with tab2:
        st.subheader("æµ‹è¯•é¢˜æ±‡æ€»")
        df_case_summary = get_case_summary_stats()
        if not df_case_summary.empty:
            for _, row in df_case_summary.iterrows():
                with st.container(border=True):
                    col1, col2, col3 = st.columns([3, 2, 1])
                    col1.write(f"**æµ‹è¯•é¢˜: {row['case_title']}**")
                    col2.write(f"å…¨æ¨¡å‹å¹³å‡åˆ†: **{row['avg_score']:.2f}** / 100")
                    col3.write(f"æ€»è¿è¡Œæ¬¡æ•°: {row['total_runs']}")

                    if st.button("æŸ¥çœ‹æ¨¡å‹æ’å", key=f"case_rank_{row['case_id']}"):
                        st.session_state[f"show_rank_{row['case_id']}"] = not st.session_state.get(f"show_rank_{row['case_id']}", False)
                        st.rerun()

                    if st.session_state.get(f"show_rank_{row['case_id']}", False):
                        st.write("---")
                        df_ranking = get_case_model_ranking(row['case_id'])
                        st.table(df_ranking.rename(columns={
                            'model_name': 'æ¨¡å‹åç§°',
                            'avg_score': 'å¹³å‡åˆ†',
                            'run_count': 'è¿è¡Œæ¬¡æ•°'
                        }))
        else:
            st.info("æš‚æ— æµ‹è¯•é¢˜ç»Ÿè®¡æ•°æ®ã€‚")

    with tab3:
        st.subheader("â±ï¸ æ¨¡å‹å¹³å‡è€—æ—¶æ’è¡Œ (æ¯«ç§’)")
        df_speed = get_model_speed_ranking()
        if not df_speed.empty:
            st.dataframe(
                df_speed.rename(columns={
                    'model_name': 'æ¨¡å‹åç§°',
                    'avg_total_time_ms': 'å¹³å‡æ€»è€—æ—¶ (æ¯«ç§’)',
                    'avg_tps': 'å¹³å‡ç”Ÿæˆé€Ÿåº¦ (TPS)',
                    'avg_prompt_tps': 'å¹³å‡é¢„è¯»é€Ÿåº¦ (TPS)',
                    'test_count': 'æµ‹è¯•æ¬¡æ•°'
                }),
                width='stretch',
                hide_index=True
            )

            st.bar_chart(df_speed.set_index('model_name')['avg_total_time_ms'])
        else:
            st.info("æš‚æ— é€Ÿåº¦ç»Ÿè®¡æ•°æ®ã€‚")
